{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc467c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############import#################\n",
    "import h5py\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(33)\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "# from tensorflow import keras\n",
    "from tensorflow.keras import layers,models,optimizers,callbacks,constraints\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D,multiply,Lambda,Add,Concatenate, Multiply,Conv2DTranspose,Layer, Reshape, ZeroPadding2D,Flatten, MaxPooling2D, UpSampling2D, BatchNormalization,LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,Callback,LearningRateScheduler,ModelCheckpoint\n",
    "# set_session(tf.Session(config=config))\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "import os\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "jobid=int(0)\n",
    "param={i:[] for i in range(11)}\n",
    "param[0]=[2,1000,5,1000,500,15,'ppp']###################\n",
    "\n",
    "param[1]=[3,1,1000,1,10,20,'sr']\n",
    "param[2]=[2,1000,10,1,10,20,'sr']###\n",
    "param[3]=[2,0.001,1,1,5,20,'sr']###\n",
    "param[4]=[2,0.001,5,1,5,30,'sr']\n",
    "param[5]=[2,0.001,10,1,10,20,'sr']####\n",
    "param[6]=[3,100,5,1,10,20,'sr']###\n",
    "#############hyperparameter#################\n",
    "background=2\n",
    "udim=param[jobid][0]###\n",
    "sdim=5\n",
    "latent_dim=background+udim+sdim\n",
    "bac=2\n",
    "alpha=param[jobid][1]###\n",
    "beta=param[jobid][2]####\n",
    "gamma=param[jobid][3]####\n",
    "theta=param[jobid][4]####\n",
    "ks=param[jobid][5]####\n",
    "flag=param[jobid][6]####\n",
    "batch_size=10\n",
    "\n",
    "hdf5_file = 'data_nv30364657.hdf5' #Data available at: https://drive.google.com/file/d/1efgnHQ9ZXahn3-Z-boX9awmdq04XFKmZ/view?usp=sharing\n",
    "\n",
    "a = h5py.File(hdf5_file, 'r')\n",
    "\n",
    "name=list(a.keys())\n",
    "i1=a[name[1]]\n",
    "trail=list(i1.keys())\n",
    "# i12=i1[trail[1]]\n",
    "# # i12\n",
    "image_name=['images_30', 'images_36', 'images_46', 'images_57']\n",
    "label_name=['labels_30', 'labels_36', 'labels_46', 'labels_57']\n",
    "# print(name)\n",
    "va={i:[] for i in range (4)}\n",
    "\n",
    "for i in range (4):\n",
    "    imn=image_name[i]\n",
    "    lbn=image_name[i]\n",
    "    for j in trail:\n",
    "#         print(imn)\n",
    "        data=np.array(a[imn][j])\n",
    "        v=np.var(data,axis=0)\n",
    "        va[i].append(np.mean(v))\n",
    "\n",
    "so={i:[] for i in range (4)}\n",
    "vv={i:[] for i in range (4)}\n",
    "for i in range (4):\n",
    "    so[i]=np.argsort(np.array(va[i])*-1)\n",
    "    vv[i]=np.sort(np.array(va[i])*-1)\n",
    "\n",
    "\n",
    "z=0\n",
    "idx={i:[] for i in range (388*2*189)}\n",
    "for i in range(4):\n",
    "    seq=so[i]\n",
    "    for j in seq[:194]:\n",
    "        for k in range (189):\n",
    "            n=trail[j]\n",
    "            idx[z].append(i)\n",
    "            idx[z].append(n)\n",
    "            idx[z].append(k)\n",
    "            z+=1\n",
    "\n",
    "idx2=shuffle(idx)\n",
    "\n",
    "\n",
    "idx={i:[] for i in range (388*2*189)}\n",
    "z=0\n",
    "for k in range (189):\n",
    "    for i in range(4):\n",
    "        seq=so[i]\n",
    "        for j in seq[int(len(trail)/2):]:\n",
    "            n=trail[j]\n",
    "    #         print(n)\n",
    "\n",
    "            idx[z].append(i)\n",
    "            idx[z].append(n)\n",
    "            idx[z].append(k)\n",
    "            z+=1\n",
    "# len(idx)\n",
    "idx3=shuffle(idx)\n",
    "index=idx2+idx3\n",
    "\n",
    "def data_generator(idx, hdf5file,batch_size,if_train = True):\n",
    "    i = 0\n",
    "#     j=0 \n",
    "    while True:\n",
    "        X = []\n",
    "        Y=[]\n",
    "        for b in range(batch_size):\n",
    "            if i == len(idx):\n",
    "                i = 0\n",
    "            \n",
    "            alll=idx[i]\n",
    "            if alll[0]==0:\n",
    "                imn='images_30'\n",
    "                lbn='labels_30'\n",
    "\n",
    "            elif alll[0]==1:\n",
    "                imn='images_36'\n",
    "                lbn='labels_36'\n",
    "            elif alll[0]==2:\n",
    "                imn='images_46'\n",
    "                lbn='labels_46'\n",
    "            else:\n",
    "                imn='images_57'\n",
    "                lbn='labels_57'\n",
    "                \n",
    "            try:\n",
    "                x = (np.array(hdf5file[imn][alll[1]][alll[2]])/255).T  # read dataset on the fly\n",
    "                x=np.rollaxis(x,1,0)\n",
    "\n",
    "                y = np.array(hdf5file[lbn][alll[1]][alll[2]])  # read dataset on the fly\n",
    "\n",
    "\n",
    "                if np.sum(x)==0 or np.sum(x) == np.inf or np.sum(x) == -np.inf or np.sum(y)==0 or np.sum(y)==np.inf or np.sum(y)==-np.inf:\n",
    "                    x = (np.array(hdf5file[imn]['trial_0030'][60])/255).T  # read dataset on the fly\n",
    "                    x=np.rollaxis(x,1,0)\n",
    "                    y = np.array(hdf5file[lbn]['trial_0030'][60])  # read dataset on the fly\n",
    "            except:\n",
    "                x = (np.array(hdf5file[imn]['trial_0030'][60])/255).T  # read dataset on the fly\n",
    "                x=np.rollaxis(x,1,0)\n",
    "                y = np.array(hdf5file[lbn]['trial_0030'][60])  # read dataset on the fly\n",
    "            \n",
    "\n",
    "                \n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "\n",
    "            i += 1\n",
    "#             j += 1\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        Y = np.asarray(Y)\n",
    "        yield [X,Y],[X,Y]\n",
    "        \n",
    "# idx=np.arange(len(image))\n",
    "###############################################\n",
    "##################define classes for loss and regularizor######\n",
    "def Distance(x, y):\n",
    "    d = tf.reduce_sum(tf.square(x - y), 1)\n",
    "    dx = tf.maximum(d, 1e-9)\n",
    "    dist = tf.sqrt(dx)\n",
    "    return tf.reduce_mean(dist)\n",
    "\n",
    "\n",
    "def swiss_roll(size, noise=1.5):\n",
    "    u = K.random_uniform(shape=(size[0], 1),dtype=\"float32\")\n",
    "    u2 = K.random_uniform(shape=(size[0], 1),dtype=\"float32\")*10\n",
    "\n",
    "    t = 1.5 * np.pi * (1 + 1.5* u)\n",
    "    x = t * K.cos(t)#+11*K.ones_like(u2)\n",
    "    y =  u2 #+ 11*K.ones_like(u2)\n",
    "    z = t * K.sin(t) #+ 11*K.ones_like(u2)\n",
    "\n",
    "    X = K.concatenate([x,z], axis=-1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def squre_roll(size, noise=1.5):\n",
    "    u =K.random_uniform(shape=(size[0], 1),dtype=\"float32\")\n",
    "    u2 = K.random_uniform(shape=(size[0], 1),dtype=\"float32\")*10\n",
    "#     print(u)\n",
    "    t = 1.5 * np.pi * (1 + 100 * u)\n",
    "    x = t * K.cos(t)#+11*K.ones_like(u2)\n",
    "    y =  u2# + 11*K.ones_like(u2)\n",
    "    z = t * K.sin(t) #+ 11*K.ones_like(u2)\n",
    "\n",
    "    X = K.concatenate([x,y,z], axis=-1)\n",
    "\n",
    "    return X\n",
    "def spherical(size, ndim=3):\n",
    "    vec = K.random_normal(shape=(size[0], ndim),dtype=\"float32\")\n",
    "#     vec /= K.linalg.norm(vec, axis=0)\n",
    "    return vec\n",
    "\n",
    "def clusterdis(size, noise=1.5):\n",
    "    \n",
    "    a=int(size[0]/4)\n",
    "    b=a\n",
    "    c=a\n",
    "    d=size[0]-a-b-c\n",
    "    u = K.random_uniform(shape=(a, 3),dtype=\"float32\")\n",
    "    u2 = K.random_uniform(shape=(b, 3),dtype=\"float32\")*2\n",
    "    u3 = K.random_uniform(shape=(c, 3),dtype=\"float32\")*4\n",
    "    u4 = K.random_uniform(shape=(d, 3),dtype=\"float32\")*8\n",
    "#     sess = tf.Session()\n",
    "#     with sess.as_default():\n",
    "#         t=K.eval(size[0])\n",
    "    X = K.concatenate([u,u2,u3,u4], axis=0)\n",
    "\n",
    "#     X,_=make_blobs(n_samples=t, centers=4, n_features=3,random_state=42)\n",
    "    return X\n",
    "\n",
    "def GMM(size,noise=1.5):\n",
    "    \n",
    "    N,D = size[0], 3 # number of points and dimenstinality\n",
    "\n",
    "    means = np.array([[0.5, 0.0, 0.0],\n",
    "                      [0.0, 0.5, 0.8],\n",
    "                      [-0.5, -0.5, -0.5],\n",
    "                      [-0.8, 0.3, 0.4]])\n",
    "#     means= K.constant(means)\n",
    "\n",
    "    covs = np.array([np.diag([0.01, 0.01, 0.01]),\n",
    "                     np.diag([0.01, 0.01, 0.01]),\n",
    "                     np.diag([0.01, 0.01, 0.01]),\n",
    "                     np.diag([0.01, 0.01, 0.01])])\n",
    "#     covs=K.constant(covs)\n",
    "\n",
    "    n_gaussians = means.shape[0]\n",
    "\n",
    "    points = []\n",
    "    if N % 4 == 0:\n",
    "        L=int(N/4)\n",
    "        \n",
    "        \n",
    "    for i in range(len(means)):\n",
    "        x = np.random.multivariate_normal(means[i], covs[i],L)\n",
    "        points.append(x)\n",
    "\n",
    "    points=K.constant(np.concatenate(points))\n",
    "    \n",
    "    return points\n",
    "\n",
    "def get_kernel(X, Z,ksize):\n",
    "#     print(X.shape,Z.shape)\n",
    "    G = K.sum((K.expand_dims(X, axis=1) - Z)**2, axis=-1)  # Gram matrix\n",
    "    G = K.exp(-G/(ksize)) / (math.sqrt(2*np.pi*ksize)*K.ones_like(-G/(ksize)))\n",
    "    return G\n",
    "\n",
    "class DiagonalWeight(Constraint):\n",
    "    \"\"\"Constrains the weights to be diagonal.\n",
    "    \"\"\"\n",
    "    def __init__(self, N):\n",
    "        self.m = K.eye(N)\n",
    "        \n",
    "    def __call__(self, w):\n",
    "#         N = K.int_shape(w)[-1]\n",
    "#         m = K.eye(N)\n",
    "        w = w*self.m\n",
    "        return w\n",
    "    \n",
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch =  -0.5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) - K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(tf.reduce_mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs,tf.reduce_mean(kl_batch)\n",
    "\n",
    "class ITLRegularizer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds cs divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(ITLRegularizer, self).__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "    \n",
    "    def call(self, inputs,ks,flag,theta):\n",
    "\n",
    "        X = inputs\n",
    "#         print(X)\n",
    "        if flag=='sp':\n",
    "            Z=spherical(K.shape(X))\n",
    "        elif flag == 'cb':\n",
    "            Z=clusterdis(K.shape(X))\n",
    "        elif flag == 'gmm':\n",
    "        \n",
    "            Z=GMM(K.shape(X))\n",
    "        elif flag == 'sq':\n",
    "            Z=squre_roll(K.shape(X))\n",
    "        else:\n",
    "            Z=swiss_roll(K.shape(X))\n",
    "\n",
    "    \n",
    "\n",
    "#         Z=Z.astype(\"float32\")\n",
    "        ksize=ks\n",
    "        Gxx = get_kernel(X, X,ksize)\n",
    "        Gzz = get_kernel(Z, Z,ksize)\n",
    "        Gxz = get_kernel(X, Z,ksize)\n",
    "\n",
    "\n",
    "        r=K.log(K.sqrt(K.mean(Gxx)*K.mean(Gzz)+1e-5) /(K.mean(Gxz)+1e-5))\n",
    "\n",
    "        self.add_loss(r*theta, inputs=inputs)\n",
    "\n",
    "        return inputs,Z,r*theta\n",
    "    \n",
    "class OrthLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(OrthLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs,gamma):\n",
    "\n",
    "        A,B = inputs\n",
    "        U=K.concatenate((A,B),axis=1)\n",
    "        U = K.l2_normalize(U, axis=1)\n",
    "        batch = -K.dot(K.transpose(U),U) \n",
    "        I =tf.eye(K.shape(U)[1])#tf.Variable(lambda:K.eye(K.shape(U)[1])) #Lambda(lambda t: K.eye(t))()\n",
    "        batch=(batch+I)**2*gamma\n",
    "        self.add_loss(tf.reduce_mean(batch), inputs=inputs)\n",
    "\n",
    "        return inputs,tf.reduce_mean(batch)\n",
    "\n",
    "    \n",
    "    \n",
    "class MSE_SUP(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(MSE_SUP, self).__init__(*args, **kwargs)\n",
    "    def call(self,inputs, alpha):\n",
    "        D,A=inputs\n",
    "        L=tf.keras.losses.mse(D,A)\n",
    "        L=tf.reduce_mean(L)\n",
    "        \n",
    "        self.add_loss(L*alpha, inputs=inputs)\n",
    "        \n",
    "        return inputs,L*alpha\n",
    "        \n",
    "    \n",
    "class MSE_UNSUP(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(MSE_UNSUP, self).__init__(*args, **kwargs)\n",
    "    def call(self,inputs):\n",
    "        D,A=inputs\n",
    "\n",
    "        L=tf.keras.losses.mse(D,A)\n",
    "        L=tf.reduce_mean(L)\n",
    "        self.add_loss(L*128*128*2, inputs=inputs)\n",
    "        \n",
    "        return inputs,L*128*128*2\n",
    "    \n",
    "    \n",
    "LN2PI=np.log(2 * 3.1415926)\n",
    "\n",
    "\n",
    "\n",
    "class De_KL(Layer):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(De_KL, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def _gaussian_log_density_unsummed(self,z, mu, logvar):\n",
    "        \"\"\"First step of Gaussian log-density computation, without summing over dimensions.\n",
    "        Assumes a diagonal noise covariance matrix.\n",
    "        \"\"\"\n",
    "        diff_sq = (z - mu) ** 2\n",
    "        inv_var = K.exp(-logvar)\n",
    "        return - 0.5 * (inv_var * diff_sq + logvar + LN2PI)\n",
    "\n",
    "\n",
    "    def _gaussian_log_density_unsummed_std_normal(self,z):\n",
    "        \"\"\"First step of Gaussian log-density computation, without summing over dimensions.\n",
    "        Assumes a diagonal noise covariance matrix.\n",
    "        \"\"\"\n",
    "        diff_sq = z ** 2\n",
    "        return - 0.5 * (diff_sq + LN2PI)\n",
    "\n",
    "    \n",
    "    def _logsumexp(self,x, axis=None, keepdims=False):\n",
    "\n",
    "        x = tf.convert_to_tensor(x)\n",
    "        x_max = tf.math.reduce_max(x, axis=axis, keepdims=False)\n",
    "        temp=tf.math.reduce_sum(K.exp(x - x_max)+1, axis=axis,keepdims=True)\n",
    "        ret = K.log(tf.math.reduce_max(temp+1,tf.zeros_like(temp+1))) + x_max\n",
    "#         np.log(np.max(x, 1e-9))\n",
    "        if not keepdims:\n",
    "            ret = tf.math.reduce_sum(ret, axis=axis)\n",
    "#         print(K.sum(x,axis=1).numpy())\n",
    "        return ret#K.sum(x,axis=1)\n",
    "\n",
    "    \n",
    "    def call(self,inputs,beta):\n",
    "        z, mu, logvar=inputs\n",
    "        log_qz_prob = self._gaussian_log_density_unsummed(z[:, None], mu[None, :], logvar[None, :])\n",
    "        M =tf.nn.relu(K.sum(log_qz_prob, axis=2, keepdims=False))#,axis=1,keepdims=False)\n",
    "        c =tf.nn.relu(log_qz_prob)#,axis=1,keepdims=False)\n",
    "        t1=K.sum(K.exp(-M)+K.exp(K.sum(log_qz_prob, axis=2, keepdims=False)-M),axis=1,keepdims=False)\n",
    "        log_qz= K.log(t1+1e-5)\n",
    "        #tf.reduce_logsumexp(K.sum(log_qz_prob, axis=2, keepdims=False),axis=1,keepdims=False)\n",
    "       \n",
    "        \n",
    "        #self._logsumexp(K.sum(log_qz_prob, axis=2, keepdims=False),axis=1,keepdims=False)\n",
    "        \n",
    "        \n",
    "#         print(log_qz.shape)\n",
    "        \n",
    "        log_qz_ = tf.linalg.diag(M) # sum over gaussian dims\n",
    "        \n",
    "#         print(log_qz_.shape)\n",
    "        t2=K.sum(K.exp(-c)+K.exp(log_qz_prob-c),axis=1,keepdims=False)\n",
    "        log_qz_product = K.sum(#log_qz_prob,\n",
    "#             K.sum(K.log(K.exp(-c)+K.exp(log_qz_prob-c)),axis=1,keepdims=False),\n",
    "#             self._logsumexp(log_qz_prob, axis=1,keepdims=False),\n",
    "            K.log(t2+1e-5),\n",
    "#             tf.reduce_logsumexp(log_qz_prob, axis=1,keepdims=False),  # logsumexp over batch\n",
    "            axis=1,  # sum over gaussian dims\n",
    "            keepdims=False)\n",
    "        \n",
    "        log_pz_prob = self._gaussian_log_density_unsummed_std_normal(z)\n",
    "        log_pz_product = K.sum(log_pz_prob, axis=1, keepdims=False)  # sum over gaussian dims\n",
    "#         print(log_qz_.shape , log_qz.shape)\n",
    "        idx_code_mi = tf.experimental.numpy.nanmean(log_qz_ - log_qz)\n",
    "        total_corr = tf.experimental.numpy.nanmean(log_qz - log_qz_product)\n",
    "        dim_wise_kl = tf.experimental.numpy.nanmean(log_qz_product - log_pz_product)\n",
    "        idx_code_mi = tf.reduce_mean(idx_code_mi )\n",
    "        total_corr = tf.reduce_mean(total_corr )\n",
    "        dim_wise_kl = tf.reduce_mean(dim_wise_kl)\n",
    "        self.add_loss((idx_code_mi+total_corr *beta+dim_wise_kl+1e-5), inputs=inputs)\n",
    "        \n",
    "        return inputs,[idx_code_mi,total_corr *beta,dim_wise_kl]\n",
    "###############################################\n",
    "input0 = Input(shape=(128,128,2), name=\"mice1\")\n",
    "# encoder=Sequential()\n",
    "inputlabel=Input(shape=(sdim), name=\"micelabel1\")\n",
    "\n",
    "encoder=LeakyReLU(alpha=0.05)(Conv2D(32, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")(input0))\n",
    "encoder=BatchNormalization()(encoder)\n",
    "# print(encoder.shape)\n",
    "encoder=LeakyReLU(alpha=0.05)(Conv2D(64, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")(encoder))\n",
    "encoder=BatchNormalization()(encoder)\n",
    "# print(encoder.shape)\n",
    "encoder=LeakyReLU(alpha=0.05)(Conv2D(128, (5,5), strides=(2,2),padding='same', kernel_initializer=\"he_normal\")(encoder))\n",
    "encoder=BatchNormalization()(encoder)\n",
    "# print(encoder.shape)\n",
    "encoder=LeakyReLU(alpha=0.05)(Conv2D(256, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")(encoder))\n",
    "encoder=BatchNormalization()(encoder)\n",
    "# print(encoder.shape)\n",
    "encoder=LeakyReLU(alpha=0.05)(Conv2D(512, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")(encoder))\n",
    "encoder=BatchNormalization()(encoder)\n",
    "# print(encoder.shape)\n",
    "hidden=Flatten()(encoder)\n",
    "# hidden.shape\n",
    "\n",
    "z_mu = Dense(latent_dim,kernel_regularizer='l1_l2')(hidden)\n",
    "z_log_var = Dense(sdim+udim,kernel_regularizer='l1_l2')(hidden)\n",
    "#\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "m = ortho_group.rvs(dim=latent_dim).astype('float32')\n",
    "\n",
    "from scipy.linalg import qr\n",
    "\n",
    "n = sdim+udim+bac\n",
    "H = np.random.randn(n, n)\n",
    "Q, _ = qr(H)\n",
    "\n",
    "initializer = tf.keras.initializers.Orthogonal()\n",
    "initializera=tf.keras.initializers.Orthogonal(gain=1.0, seed=42)\n",
    "initializerb=tf.keras.initializers.Orthogonal(gain=2.0, seed=30)\n",
    "\n",
    "\n",
    "class FixWeights_sup(tf.keras.constraints.Constraint):\n",
    "\n",
    "    def __call__(self, w):\n",
    "        tf.keras.backend.set_value(w, Q[:sdim,:sdim])\n",
    "        return w\n",
    "class FixWeights_unsup(tf.keras.constraints.Constraint):\n",
    "\n",
    "    def __call__(self, w):\n",
    "        tf.keras.backend.set_value(w, Q[sdim:sdim+udim,sdim:sdim+udim])\n",
    "        return w\n",
    "    \n",
    "class FixWeights_bac(tf.keras.constraints.Constraint):\n",
    "\n",
    "    def __call__(self, w):\n",
    "        tf.keras.backend.set_value(w, Q[sdim+udim:sdim+udim+bac-1,sdim+udim:sdim+udim+bac-1])\n",
    "        return w\n",
    "\n",
    "C1 = Dense(bac, use_bias=False,kernel_regularizer='l1_l2')#(z_mu)\n",
    "A1 = Dense(sdim,use_bias=False,kernel_initializer=initializera,kernel_regularizer='l1_l2')#(z_mu)\n",
    "B1 = Dense(udim, use_bias=False, kernel_initializer=initializerb,kernel_regularizer='l1_l2')#(z_mu)\n",
    "\n",
    "\n",
    "z_log_var_A=z_log_var[:,:sdim]\n",
    "z_log_var_B=z_log_var[:,sdim:]\n",
    "\n",
    "\n",
    "\n",
    "A1.trainable = False\n",
    "B1.trainable = False\n",
    "\n",
    "A2 = A1(z_mu)\n",
    "B = B1(z_mu)\n",
    "C = C1(z_mu)\n",
    "\n",
    "C,dis,loss2= ITLRegularizer()(C,ks,flag,theta)\n",
    "\n",
    "D1=Dense(sdim,use_bias=True,kernel_constraint=DiagonalWeight(sdim), kernel_initializer=\"he_normal\")(A2)\n",
    "[D,A3],loss3=MSE_SUP()([D1,inputlabel],alpha)\n",
    "\n",
    "[A, z_log_var_A2],loss4 = KLDivergenceLayer()([A2, z_log_var[:,:sdim]])\n",
    "z_sigma_A = Lambda(lambda t: K.exp(.5*t))(z_log_var_A2)\n",
    "\n",
    "eps_A = K.random_normal(shape=(K.shape(input0)[0],sdim))\n",
    "z_eps_A2 = Multiply()([z_sigma_A, eps_A])#Multiply()([z_sigma_A, eps_A])\n",
    "z_A = Add()([A, z_eps_A2])\n",
    "\n",
    "z_sigma_B = Lambda(lambda t: K.exp(.5*t))(z_log_var[:,sdim:])\n",
    "eps_B = K.random_normal(shape=(K.shape(input0)[0],udim))\n",
    "z_eps_B = Multiply()([z_sigma_B, eps_B])\n",
    "z_B = Add()([B, z_eps_B])\n",
    "\n",
    "[z_B1, B1, z_log_var_B1],[loss5,loss6,loss7]= De_KL()([z_B, B, z_log_var_B],beta)\n",
    "\n",
    "\n",
    "alla=Concatenate()([z_A,z_B1,C])\n",
    "\n",
    "\n",
    "# print(zall.shape)\n",
    "decoder = Sequential()\n",
    "decoder.add(Dense(hidden.shape[1], kernel_initializer=\"he_normal\"))\n",
    "decoder.add(Reshape((4, 4, 512)))\n",
    "decoder.add(Conv2DTranspose(256, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\"))\n",
    "decoder.add(LeakyReLU(alpha=0.05))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "decoder.add(Conv2DTranspose(128, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\"))\n",
    "decoder.add(LeakyReLU(alpha=0.05))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "decoder.add(Conv2DTranspose(64, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\"))\n",
    "decoder.add(LeakyReLU(alpha=0.05))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "decoder.add(Conv2DTranspose(32, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\"))\n",
    "decoder.add(LeakyReLU(alpha=0.05))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "decoder.add(Conv2DTranspose(2, (5,5), strides= (2,2),activation='sigmoid',padding='same', kernel_initializer=\"he_normal\"))\n",
    "\n",
    "encmodel = Model(inputs=[input0,inputlabel], outputs=alla)\n",
    "\n",
    "out=decoder(alla)\n",
    "[out,_],loss8=MSE_UNSUP()([out,input0])\n",
    "\n",
    "\n",
    "\n",
    "allmodel= Model(inputs=[input0,inputlabel], outputs=[out,D])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001,clipvalue=1.0)\n",
    "\n",
    "allmodel.add_metric(loss2, \"csd_loss\")\n",
    "allmodel.add_metric(loss3, \"sup_loss\")\n",
    "allmodel.add_metric(loss4, \"kl_loss\")\n",
    "allmodel.add_metric(loss5, \"mi_loss\")\n",
    "allmodel.add_metric(loss6, \"total_corr\")\n",
    "allmodel.add_metric(loss7, \"dim_wise_kl\")\n",
    "allmodel.add_metric(loss8, \"UNSUP_loss\")\n",
    "\n",
    "allmodel.compile( optimizer=optimizer)\n",
    "\n",
    "\n",
    "callbacks=[LearningRateScheduler(lambda epoch: 0.001 * 0.85 ** (epoch // 5))]\n",
    "term=tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "\n",
    "\n",
    "history1 =allmodel.fit( data_generator(index, a,256,True),validation_data=data_generator(index[-10000:],a,100,False),\n",
    "                       batch_size=256, epochs=50,validation_steps=np.ceil(10000/100-1),\n",
    "                       verbose=1, steps_per_epoch=np.ceil((len(index))/256-1),\n",
    "                       callbacks=[term,callbacks])\n",
    "\n",
    "allmodel.save_weights('model.h5')\n",
    "\n",
    "# allmodel.load_weights('csdmodel_4_udim=2_alpha=1000_beta=5_gamma=1000_theta=500_ks=15_flag=sror3.h5')###!!!!!!!\n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
