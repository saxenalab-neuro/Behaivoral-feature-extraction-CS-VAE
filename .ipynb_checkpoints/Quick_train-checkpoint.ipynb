{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc467c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 19s 7s/step - loss: 218802.6875 - csd_loss: 1362.7295 - sup_loss: 210880.9375 - kl_loss: 729.2978 - mi_loss: -5.5548 - total_corr: -28.4358 - dim_wise_kl: 927.8747 - UNSUP_loss: 4917.4189 - val_loss: 7838.3940 - val_csd_loss: 898.3448 - val_sup_loss: 1292.1251 - val_kl_loss: 3.3378 - val_mi_loss: -4.6677 - val_total_corr: -25.0165 - val_dim_wise_kl: 14.9311 - val_UNSUP_loss: 5640.9111\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 79926.0312 - csd_loss: 1784.0203 - sup_loss: 73476.7031 - kl_loss: 451.0207 - mi_loss: -5.5605 - total_corr: -28.2723 - dim_wise_kl: 835.3083 - UNSUP_loss: 3394.3813 - val_loss: 13452.5459 - val_csd_loss: 1139.3817 - val_sup_loss: 3106.1533 - val_kl_loss: 19.8895 - val_mi_loss: -4.6523 - val_total_corr: -24.6882 - val_dim_wise_kl: 14.2709 - val_UNSUP_loss: 9183.8018\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 26245.2715 - csd_loss: 719.1708 - sup_loss: 22377.9980 - kl_loss: 472.3616 - mi_loss: -5.5744 - total_corr: -28.2092 - dim_wise_kl: 238.3503 - UNSUP_loss: 2452.7910 - val_loss: 14916.1113 - val_csd_loss: 857.3939 - val_sup_loss: 1960.9779 - val_kl_loss: 46.2936 - val_mi_loss: -4.6333 - val_total_corr: -24.3868 - val_dim_wise_kl: 26.1758 - val_UNSUP_loss: 12035.9277\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 9271.4453 - csd_loss: 547.4922 - sup_loss: 6307.1836 - kl_loss: 125.5190 - mi_loss: -5.5802 - total_corr: -28.4934 - dim_wise_kl: 43.1792 - UNSUP_loss: 2263.7837 - val_loss: 17287.5879 - val_csd_loss: 800.0552 - val_sup_loss: 2768.7554 - val_kl_loss: 335.6555 - val_mi_loss: -4.6267 - val_total_corr: -24.2709 - val_dim_wise_kl: 36.2885 - val_UNSUP_loss: 13357.3740\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 6761.1416 - csd_loss: 363.1121 - sup_loss: 4190.3164 - kl_loss: 87.5211 - mi_loss: -5.5690 - total_corr: -28.4650 - dim_wise_kl: 94.2950 - UNSUP_loss: 2041.5826 - val_loss: 24801.7676 - val_csd_loss: 985.0969 - val_sup_loss: 3516.8945 - val_kl_loss: 6320.4868 - val_mi_loss: -4.6231 - val_total_corr: -24.4452 - val_dim_wise_kl: 50.9871 - val_UNSUP_loss: 13939.0176\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 3916.0347 - csd_loss: 389.9296 - sup_loss: 1403.7207 - kl_loss: 32.9393 - mi_loss: -5.5584 - total_corr: -28.3644 - dim_wise_kl: 39.3969 - UNSUP_loss: 2065.6272 - val_loss: 27270.0098 - val_csd_loss: 628.4006 - val_sup_loss: 3106.5444 - val_kl_loss: 9670.7119 - val_mi_loss: -4.6214 - val_total_corr: -24.4837 - val_dim_wise_kl: 105.3068 - val_UNSUP_loss: 13769.8027\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 3477.6597 - csd_loss: 332.5666 - sup_loss: 1118.9794 - kl_loss: 60.8551 - mi_loss: -5.5486 - total_corr: -28.1564 - dim_wise_kl: 90.2657 - UNSUP_loss: 1890.3533 - val_loss: 29971.8359 - val_csd_loss: 541.4289 - val_sup_loss: 3550.2832 - val_kl_loss: 11944.6465 - val_mi_loss: -4.6184 - val_total_corr: -23.8637 - val_dim_wise_kl: 138.8667 - val_UNSUP_loss: 13806.7510\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 3561.0493 - csd_loss: 308.2775 - sup_loss: 883.0932 - kl_loss: 491.8106 - mi_loss: -5.5468 - total_corr: -28.2610 - dim_wise_kl: 46.2092 - UNSUP_loss: 1847.1201 - val_loss: 43328.8398 - val_csd_loss: 459.1812 - val_sup_loss: 3327.2739 - val_kl_loss: 25285.8359 - val_mi_loss: -4.6225 - val_total_corr: -23.5119 - val_dim_wise_kl: 203.7100 - val_UNSUP_loss: 14062.6387\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 3141.3745 - csd_loss: 308.3725 - sup_loss: 740.0658 - kl_loss: 15.1957 - mi_loss: -5.5457 - total_corr: -28.2242 - dim_wise_kl: 69.8035 - UNSUP_loss: 2023.3602 - val_loss: 122635.0938 - val_csd_loss: 477.7274 - val_sup_loss: 2308.1523 - val_kl_loss: 104877.5547 - val_mi_loss: -4.6240 - val_total_corr: -23.3179 - val_dim_wise_kl: 638.3455 - val_UNSUP_loss: 14342.9082\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 3137.1128 - csd_loss: 244.1911 - sup_loss: 955.4651 - kl_loss: 19.7531 - mi_loss: -5.5455 - total_corr: -28.2926 - dim_wise_kl: 79.9877 - UNSUP_loss: 1853.2080 - val_loss: 1219443.5000 - val_csd_loss: 507.5396 - val_sup_loss: 2825.1433 - val_kl_loss: 1201551.0000 - val_mi_loss: -4.6264 - val_total_corr: -23.2812 - val_dim_wise_kl: 275.2459 - val_UNSUP_loss: 14293.9443\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 2830.7012 - csd_loss: 238.0152 - sup_loss: 733.0167 - kl_loss: 20.4881 - mi_loss: -5.5453 - total_corr: -28.2195 - dim_wise_kl: 59.3635 - UNSUP_loss: 1795.2410 - val_loss: 2508008.5000 - val_csd_loss: 662.7933 - val_sup_loss: 4201.8594 - val_kl_loss: 2489150.7500 - val_mi_loss: -4.6239 - val_total_corr: -23.4362 - val_dim_wise_kl: 254.3773 - val_UNSUP_loss: 13748.5898\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 2656.1572 - csd_loss: 210.8373 - sup_loss: 664.3168 - kl_loss: 14.7826 - mi_loss: -5.5452 - total_corr: -27.9504 - dim_wise_kl: 59.9160 - UNSUP_loss: 1721.4622 - val_loss: 1682207.5000 - val_csd_loss: 509.7091 - val_sup_loss: 2512.0103 - val_kl_loss: 1665340.6250 - val_mi_loss: -4.6190 - val_total_corr: -23.6264 - val_dim_wise_kl: 173.9361 - val_UNSUP_loss: 13680.9609\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 2693.0640 - csd_loss: 255.7663 - sup_loss: 663.1605 - kl_loss: 11.0909 - mi_loss: -5.5451 - total_corr: -27.8111 - dim_wise_kl: 35.3874 - UNSUP_loss: 1742.6798 - val_loss: 552140.0625 - val_csd_loss: 574.9238 - val_sup_loss: 2784.9624 - val_kl_loss: 534978.0000 - val_mi_loss: -4.6153 - val_total_corr: -23.5920 - val_dim_wise_kl: 204.2955 - val_UNSUP_loss: 13607.7637\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 2536.3450 - csd_loss: 252.6208 - sup_loss: 573.9233 - kl_loss: 9.9544 - mi_loss: -5.5451 - total_corr: -27.7669 - dim_wise_kl: 62.7299 - UNSUP_loss: 1652.0956 - val_loss: 101405.0078 - val_csd_loss: 424.8297 - val_sup_loss: 1892.1249 - val_kl_loss: 85382.6328 - val_mi_loss: -4.6135 - val_total_corr: -23.5926 - val_dim_wise_kl: 148.1568 - val_UNSUP_loss: 13567.1367\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 2455.3101 - csd_loss: 298.8289 - sup_loss: 490.9290 - kl_loss: 8.8368 - mi_loss: -5.5451 - total_corr: -27.7635 - dim_wise_kl: 40.6928 - UNSUP_loss: 1631.0017 - val_loss: 97273.5938 - val_csd_loss: 430.3315 - val_sup_loss: 1834.8948 - val_kl_loss: 81317.7578 - val_mi_loss: -4.6121 - val_total_corr: -23.4241 - val_dim_wise_kl: 116.1847 - val_UNSUP_loss: 13584.1084\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 2323.2285 - csd_loss: 244.8495 - sup_loss: 480.7257 - kl_loss: 9.7501 - mi_loss: -5.5450 - total_corr: -27.7653 - dim_wise_kl: 44.2727 - UNSUP_loss: 1558.6166 - val_loss: 80612.1406 - val_csd_loss: 412.6773 - val_sup_loss: 2133.1011 - val_kl_loss: 64351.6406 - val_mi_loss: -4.6119 - val_total_corr: -23.3763 - val_dim_wise_kl: 128.3492 - val_UNSUP_loss: 13596.0479\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 2169.2405 - csd_loss: 217.4246 - sup_loss: 391.7417 - kl_loss: 8.7248 - mi_loss: -5.5450 - total_corr: -27.7498 - dim_wise_kl: 42.8609 - UNSUP_loss: 1523.4645 - val_loss: 31197.1270 - val_csd_loss: 448.2423 - val_sup_loss: 1736.0309 - val_kl_loss: 15538.0342 - val_mi_loss: -4.6149 - val_total_corr: -23.3992 - val_dim_wise_kl: 84.2702 - val_UNSUP_loss: 13400.2490\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 2032.7343 - csd_loss: 217.0320 - sup_loss: 292.6726 - kl_loss: 8.5243 - mi_loss: -5.5450 - total_corr: -27.7394 - dim_wise_kl: 41.5802 - UNSUP_loss: 1487.8965 - val_loss: 18234.9043 - val_csd_loss: 396.1019 - val_sup_loss: 1374.0226 - val_kl_loss: 3715.1626 - val_mi_loss: -4.6134 - val_total_corr: -23.3279 - val_dim_wise_kl: 81.8469 - val_UNSUP_loss: 12677.4023\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 1921.5110 - csd_loss: 174.7288 - sup_loss: 294.8876 - kl_loss: 10.1970 - mi_loss: -5.5450 - total_corr: -27.7420 - dim_wise_kl: 40.1090 - UNSUP_loss: 1416.5684 - val_loss: 13994.0264 - val_csd_loss: 434.0858 - val_sup_loss: 1599.6885 - val_kl_loss: 807.1609 - val_mi_loss: -4.6147 - val_total_corr: -23.3599 - val_dim_wise_kl: 60.3708 - val_UNSUP_loss: 11102.3906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1991.0813 - csd_loss: 217.2347 - sup_loss: 369.7591 - kl_loss: 10.6760 - mi_loss: -5.5450 - total_corr: -27.7480 - dim_wise_kl: 40.3920 - UNSUP_loss: 1368.0126 - val_loss: 12196.0098 - val_csd_loss: 411.3609 - val_sup_loss: 1274.3606 - val_kl_loss: 233.0397 - val_mi_loss: -4.6160 - val_total_corr: -23.3394 - val_dim_wise_kl: 59.9029 - val_UNSUP_loss: 10227.0146\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1886.8237 - csd_loss: 240.3973 - sup_loss: 302.5870 - kl_loss: 10.1176 - mi_loss: -5.5450 - total_corr: -27.7454 - dim_wise_kl: 43.5141 - UNSUP_loss: 1305.2075 - val_loss: 10171.2734 - val_csd_loss: 467.5710 - val_sup_loss: 1450.7483 - val_kl_loss: 109.1255 - val_mi_loss: -4.6177 - val_total_corr: -23.4065 - val_dim_wise_kl: 40.6722 - val_UNSUP_loss: 8112.8979\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1819.8599 - csd_loss: 225.8715 - sup_loss: 303.4600 - kl_loss: 10.4616 - mi_loss: -5.5450 - total_corr: -27.7391 - dim_wise_kl: 33.2415 - UNSUP_loss: 1261.8287 - val_loss: 8630.4561 - val_csd_loss: 448.7896 - val_sup_loss: 1157.0905 - val_kl_loss: 49.8747 - val_mi_loss: -4.6192 - val_total_corr: -23.5578 - val_dim_wise_kl: 36.8166 - val_UNSUP_loss: 6947.7852\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 1824.0748 - csd_loss: 230.8509 - sup_loss: 339.4362 - kl_loss: 10.8369 - mi_loss: -5.5450 - total_corr: -27.7390 - dim_wise_kl: 43.4753 - UNSUP_loss: 1214.4875 - val_loss: 6872.6489 - val_csd_loss: 516.5944 - val_sup_loss: 1203.2321 - val_kl_loss: 31.2657 - val_mi_loss: -4.6238 - val_total_corr: -23.5116 - val_dim_wise_kl: 29.4578 - val_UNSUP_loss: 5101.9707\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1613.1672 - csd_loss: 163.6110 - sup_loss: 225.9496 - kl_loss: 9.7730 - mi_loss: -5.5450 - total_corr: -27.7392 - dim_wise_kl: 32.4970 - UNSUP_loss: 1196.3574 - val_loss: 6465.6528 - val_csd_loss: 520.2082 - val_sup_loss: 1174.2285 - val_kl_loss: 26.1491 - val_mi_loss: -4.6249 - val_total_corr: -23.6194 - val_dim_wise_kl: 29.7436 - val_UNSUP_loss: 4725.3110\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1593.9695 - csd_loss: 155.6166 - sup_loss: 222.8054 - kl_loss: 11.0386 - mi_loss: -5.5450 - total_corr: -27.7400 - dim_wise_kl: 35.2648 - UNSUP_loss: 1184.2739 - val_loss: 5950.7871 - val_csd_loss: 532.7078 - val_sup_loss: 1220.2881 - val_kl_loss: 19.9099 - val_mi_loss: -4.6248 - val_total_corr: -23.6390 - val_dim_wise_kl: 29.6498 - val_UNSUP_loss: 4158.2476\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1558.6938 - csd_loss: 174.7315 - sup_loss: 236.0556 - kl_loss: 11.7467 - mi_loss: -5.5450 - total_corr: -27.7374 - dim_wise_kl: 36.5007 - UNSUP_loss: 1114.6953 - val_loss: 4898.1328 - val_csd_loss: 574.6286 - val_sup_loss: 1116.8459 - val_kl_loss: 8.0714 - val_mi_loss: -4.6277 - val_total_corr: -23.7239 - val_dim_wise_kl: 24.6344 - val_UNSUP_loss: 3184.0632\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1487.7905 - csd_loss: 151.7515 - sup_loss: 238.8068 - kl_loss: 11.6893 - mi_loss: -5.5450 - total_corr: -27.7351 - dim_wise_kl: 33.3554 - UNSUP_loss: 1067.2285 - val_loss: 3891.8682 - val_csd_loss: 585.8738 - val_sup_loss: 1137.9257 - val_kl_loss: 5.3301 - val_mi_loss: -4.6265 - val_total_corr: -23.7200 - val_dim_wise_kl: 23.6237 - val_UNSUP_loss: 2149.2292\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1448.7754 - csd_loss: 178.2424 - sup_loss: 222.6356 - kl_loss: 10.8051 - mi_loss: -5.5450 - total_corr: -27.7373 - dim_wise_kl: 37.6813 - UNSUP_loss: 1014.4625 - val_loss: 3614.3635 - val_csd_loss: 605.7736 - val_sup_loss: 1222.2736 - val_kl_loss: 3.9139 - val_mi_loss: -4.6284 - val_total_corr: -23.8060 - val_dim_wise_kl: 21.3779 - val_UNSUP_loss: 1771.2358\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1376.0426 - csd_loss: 169.5692 - sup_loss: 196.5102 - kl_loss: 9.7601 - mi_loss: -5.5451 - total_corr: -27.7372 - dim_wise_kl: 31.1309 - UNSUP_loss: 984.1331 - val_loss: 3312.3794 - val_csd_loss: 606.9746 - val_sup_loss: 1238.6332 - val_kl_loss: 3.8132 - val_mi_loss: -4.6287 - val_total_corr: -23.7638 - val_dim_wise_kl: 23.2771 - val_UNSUP_loss: 1449.8588\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1339.7892 - csd_loss: 205.6482 - sup_loss: 164.3058 - kl_loss: 9.8443 - mi_loss: -5.5450 - total_corr: -27.7360 - dim_wise_kl: 34.2823 - UNSUP_loss: 940.7776 - val_loss: 3341.7964 - val_csd_loss: 631.0884 - val_sup_loss: 1354.4749 - val_kl_loss: 4.3571 - val_mi_loss: -4.6300 - val_total_corr: -23.8672 - val_dim_wise_kl: 21.4306 - val_UNSUP_loss: 1340.7389\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1280.4512 - csd_loss: 178.3738 - sup_loss: 173.3778 - kl_loss: 9.9998 - mi_loss: -5.5451 - total_corr: -27.7360 - dim_wise_kl: 34.7161 - UNSUP_loss: 899.0629 - val_loss: 3167.2195 - val_csd_loss: 668.4952 - val_sup_loss: 1292.2909 - val_kl_loss: 4.0919 - val_mi_loss: -4.6301 - val_total_corr: -23.8378 - val_dim_wise_kl: 21.7333 - val_UNSUP_loss: 1190.8788\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1204.8500 - csd_loss: 141.8593 - sup_loss: 169.9109 - kl_loss: 9.2558 - mi_loss: -5.5451 - total_corr: -27.7361 - dim_wise_kl: 34.6348 - UNSUP_loss: 864.2777 - val_loss: 3247.0884 - val_csd_loss: 680.5534 - val_sup_loss: 1366.5990 - val_kl_loss: 4.5173 - val_mi_loss: -4.6312 - val_total_corr: -23.9325 - val_dim_wise_kl: 20.1867 - val_UNSUP_loss: 1185.6090\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 1160.8308 - csd_loss: 168.2237 - sup_loss: 138.0866 - kl_loss: 9.0612 - mi_loss: -5.5450 - total_corr: -27.7362 - dim_wise_kl: 34.1061 - UNSUP_loss: 826.4508 - val_loss: 3221.2131 - val_csd_loss: 684.6700 - val_sup_loss: 1404.7062 - val_kl_loss: 5.0345 - val_mi_loss: -4.6316 - val_total_corr: -23.7881 - val_dim_wise_kl: 20.8070 - val_UNSUP_loss: 1116.2383\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1100.6050 - csd_loss: 143.6203 - sup_loss: 145.5565 - kl_loss: 10.0087 - mi_loss: -5.5450 - total_corr: -27.7373 - dim_wise_kl: 37.6234 - UNSUP_loss: 778.9039 - val_loss: 3313.8403 - val_csd_loss: 711.3402 - val_sup_loss: 1449.5034 - val_kl_loss: 5.6986 - val_mi_loss: -4.6325 - val_total_corr: -23.8732 - val_dim_wise_kl: 19.2003 - val_UNSUP_loss: 1138.4360\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1085.0964 - csd_loss: 167.1677 - sup_loss: 145.5816 - kl_loss: 10.1003 - mi_loss: -5.5451 - total_corr: -27.7405 - dim_wise_kl: 32.5662 - UNSUP_loss: 744.8011 - val_loss: 3321.9119 - val_csd_loss: 730.0673 - val_sup_loss: 1485.0581 - val_kl_loss: 6.2413 - val_mi_loss: -4.6322 - val_total_corr: -23.8957 - val_dim_wise_kl: 20.0318 - val_UNSUP_loss: 1090.8829\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 1015.1111 - csd_loss: 150.4509 - sup_loss: 120.6794 - kl_loss: 10.3104 - mi_loss: -5.5451 - total_corr: -27.7402 - dim_wise_kl: 33.4585 - UNSUP_loss: 715.3418 - val_loss: 3487.3220 - val_csd_loss: 739.8435 - val_sup_loss: 1598.6780 - val_kl_loss: 7.0879 - val_mi_loss: -4.6369 - val_total_corr: -23.8224 - val_dim_wise_kl: 18.9403 - val_UNSUP_loss: 1133.0820\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 962.9355 - csd_loss: 136.5214 - sup_loss: 104.6612 - kl_loss: 10.5893 - mi_loss: -5.5451 - total_corr: -27.7403 - dim_wise_kl: 32.7210 - UNSUP_loss: 693.5810 - val_loss: 3485.5303 - val_csd_loss: 761.9438 - val_sup_loss: 1581.6605 - val_kl_loss: 7.3250 - val_mi_loss: -4.6318 - val_total_corr: -23.8782 - val_dim_wise_kl: 19.1800 - val_UNSUP_loss: 1125.7911\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 949.6736 - csd_loss: 156.8420 - sup_loss: 100.8023 - kl_loss: 10.5233 - mi_loss: -5.5451 - total_corr: -27.7460 - dim_wise_kl: 33.0619 - UNSUP_loss: 663.5970 - val_loss: 3595.6099 - val_csd_loss: 731.5067 - val_sup_loss: 1736.2119 - val_kl_loss: 8.4180 - val_mi_loss: -4.6361 - val_total_corr: -23.8806 - val_dim_wise_kl: 20.3831 - val_UNSUP_loss: 1109.4746\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 6s 6s/step - loss: 907.9347 - csd_loss: 126.6954 - sup_loss: 114.4097 - kl_loss: 10.2387 - mi_loss: -5.5451 - total_corr: -27.7432 - dim_wise_kl: 34.1009 - UNSUP_loss: 637.6483 - val_loss: 3594.4739 - val_csd_loss: 750.8497 - val_sup_loss: 1690.7687 - val_kl_loss: 8.4383 - val_mi_loss: -4.6336 - val_total_corr: -23.8791 - val_dim_wise_kl: 20.0148 - val_UNSUP_loss: 1134.7922\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 922.7820 - csd_loss: 175.2118 - sup_loss: 117.5698 - kl_loss: 10.4716 - mi_loss: -5.5451 - total_corr: -27.7464 - dim_wise_kl: 32.6878 - UNSUP_loss: 602.0100 - val_loss: 3629.7642 - val_csd_loss: 729.1547 - val_sup_loss: 1782.0842 - val_kl_loss: 9.2562 - val_mi_loss: -4.6316 - val_total_corr: -23.8808 - val_dim_wise_kl: 21.4605 - val_UNSUP_loss: 1098.2052\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 865.4763 - csd_loss: 153.4010 - sup_loss: 95.3715 - kl_loss: 11.5118 - mi_loss: -5.5451 - total_corr: -27.7477 - dim_wise_kl: 36.1077 - UNSUP_loss: 584.2626 - val_loss: 3702.4238 - val_csd_loss: 768.6169 - val_sup_loss: 1772.7439 - val_kl_loss: 9.6092 - val_mi_loss: -4.6371 - val_total_corr: -23.8784 - val_dim_wise_kl: 19.5927 - val_UNSUP_loss: 1142.2684\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 845.9922 - csd_loss: 147.6723 - sup_loss: 91.3313 - kl_loss: 11.2642 - mi_loss: -5.5451 - total_corr: -27.7454 - dim_wise_kl: 30.9995 - UNSUP_loss: 579.9086 - val_loss: 3824.4006 - val_csd_loss: 771.1670 - val_sup_loss: 1921.0343 - val_kl_loss: 10.4854 - val_mi_loss: -4.6323 - val_total_corr: -23.9137 - val_dim_wise_kl: 20.3255 - val_UNSUP_loss: 1111.8340\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 813.8263 - csd_loss: 143.1356 - sup_loss: 81.6936 - kl_loss: 10.7547 - mi_loss: -5.5451 - total_corr: -27.7451 - dim_wise_kl: 33.1923 - UNSUP_loss: 560.2424 - val_loss: 3839.0818 - val_csd_loss: 806.6220 - val_sup_loss: 1853.3438 - val_kl_loss: 10.2701 - val_mi_loss: -4.6349 - val_total_corr: -23.9099 - val_dim_wise_kl: 19.1299 - val_UNSUP_loss: 1160.1713\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 767.9395 - csd_loss: 128.7297 - sup_loss: 74.7589 - kl_loss: 10.4970 - mi_loss: -5.5451 - total_corr: -27.7482 - dim_wise_kl: 31.0446 - UNSUP_loss: 538.1144 - val_loss: 3914.9319 - val_csd_loss: 787.8228 - val_sup_loss: 1966.7068 - val_kl_loss: 10.8944 - val_mi_loss: -4.6327 - val_total_corr: -23.9501 - val_dim_wise_kl: 19.2236 - val_UNSUP_loss: 1140.7852\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 734.9717 - csd_loss: 127.9586 - sup_loss: 61.2480 - kl_loss: 10.3581 - mi_loss: -5.5451 - total_corr: -27.7519 - dim_wise_kl: 29.4434 - UNSUP_loss: 521.1814 - val_loss: 3976.3015 - val_csd_loss: 805.3121 - val_sup_loss: 1994.2184 - val_kl_loss: 11.2357 - val_mi_loss: -4.6327 - val_total_corr: -23.9720 - val_dim_wise_kl: 18.8096 - val_UNSUP_loss: 1157.2577\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 750.5955 - csd_loss: 133.8715 - sup_loss: 83.5252 - kl_loss: 10.3473 - mi_loss: -5.5451 - total_corr: -27.7487 - dim_wise_kl: 28.7952 - UNSUP_loss: 509.2792 - val_loss: 3963.4839 - val_csd_loss: 819.9345 - val_sup_loss: 1942.1233 - val_kl_loss: 11.4990 - val_mi_loss: -4.6353 - val_total_corr: -23.9954 - val_dim_wise_kl: 19.0760 - val_UNSUP_loss: 1181.4170\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 6s 6s/step - loss: 737.0710 - csd_loss: 120.7678 - sup_loss: 89.6258 - kl_loss: 11.2456 - mi_loss: -5.5451 - total_corr: -27.7530 - dim_wise_kl: 29.4637 - UNSUP_loss: 501.2027 - val_loss: 4019.0413 - val_csd_loss: 824.5839 - val_sup_loss: 1990.9968 - val_kl_loss: 11.9356 - val_mi_loss: -4.6312 - val_total_corr: -23.9595 - val_dim_wise_kl: 18.3581 - val_UNSUP_loss: 1183.6989\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 723.7783 - csd_loss: 144.9299 - sup_loss: 73.4184 - kl_loss: 10.9229 - mi_loss: -5.5451 - total_corr: -27.7475 - dim_wise_kl: 28.2854 - UNSUP_loss: 481.4578 - val_loss: 4016.1440 - val_csd_loss: 829.7787 - val_sup_loss: 2002.6600 - val_kl_loss: 12.3778 - val_mi_loss: -4.6328 - val_total_corr: -23.9863 - val_dim_wise_kl: 19.2359 - val_UNSUP_loss: 1162.6594\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 706.9420 - csd_loss: 123.6035 - sup_loss: 84.2767 - kl_loss: 11.0452 - mi_loss: -5.5451 - total_corr: -27.7485 - dim_wise_kl: 28.9264 - UNSUP_loss: 474.3343 - val_loss: 4026.5486 - val_csd_loss: 831.6837 - val_sup_loss: 2003.7172 - val_kl_loss: 12.6772 - val_mi_loss: -4.6412 - val_total_corr: -24.0253 - val_dim_wise_kl: 18.7983 - val_UNSUP_loss: 1170.2959\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 7s 6s/step - loss: 644.8803 - csd_loss: 104.2312 - sup_loss: 60.5545 - kl_loss: 11.8812 - mi_loss: -5.5451 - total_corr: -27.7546 - dim_wise_kl: 28.7348 - UNSUP_loss: 454.7367 - val_loss: 4120.4121 - val_csd_loss: 853.2414 - val_sup_loss: 2067.3347 - val_kl_loss: 13.0531 - val_mi_loss: -4.6364 - val_total_corr: -24.0298 - val_dim_wise_kl: 19.5441 - val_UNSUP_loss: 1177.8700\n"
     ]
    }
   ],
   "source": [
    "##############import#################\n",
    "import h5py\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(33)\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "# from tensorflow import keras\n",
    "from tensorflow.keras import layers,models,optimizers,callbacks,constraints\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D,multiply,Lambda,Add,Concatenate, Multiply,Conv2DTranspose,Layer, Reshape, ZeroPadding2D,Flatten, MaxPooling2D, UpSampling2D, BatchNormalization,LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,Callback,LearningRateScheduler,ModelCheckpoint\n",
    "# set_session(tf.Session(config=config))\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "import os\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "jobid=int(0)\n",
    "param={i:[] for i in range(11)}\n",
    "param[0]=[2,1000,5,1000,500,15,'ppp']###################\n",
    "\n",
    "param[1]=[3,1,1000,1,10,20,'sr']\n",
    "param[2]=[2,1000,10,1,10,20,'sr']###\n",
    "param[3]=[2,0.001,1,1,5,20,'sr']###\n",
    "param[4]=[2,0.001,5,1,5,30,'sr']\n",
    "param[5]=[2,0.001,10,1,10,20,'sr']####\n",
    "param[6]=[3,100,5,1,10,20,'sr']###\n",
    "#############hyperparameter#################\n",
    "background=2\n",
    "udim=param[jobid][0]###\n",
    "sdim=5\n",
    "latent_dim=background+udim+sdim\n",
    "bac=2\n",
    "alpha=param[jobid][1]###\n",
    "beta=param[jobid][2]####\n",
    "gamma=param[jobid][3]####\n",
    "theta=param[jobid][4]####\n",
    "ks=param[jobid][5]####\n",
    "flag=param[jobid][6]####\n",
    "batch_size=10\n",
    "\n",
    "hdf5_file = 'data/part_data.hdf5' #Data available at: https://drive.google.com/file/d/1efgnHQ9ZXahn3-Z-boX9awmdq04XFKmZ/view?usp=sharing\n",
    "\n",
    "a = h5py.File(hdf5_file, 'r')\n",
    "\n",
    "name=list(a.keys())\n",
    "i1=a[name[1]]\n",
    "trail=list(i1.keys())\n",
    "# i12=i1[trail[1]]\n",
    "# # i12\n",
    "image_name=['images_30', 'images_36', 'images_46', 'images_57']\n",
    "label_name=['labels_30', 'labels_36', 'labels_46', 'labels_57']\n",
    "# print(name)\n",
    "va={i:[] for i in range (4)}\n",
    "\n",
    "for i in range (4):\n",
    "    imn=image_name[i]\n",
    "    lbn=image_name[i]\n",
    "    for j in trail:\n",
    "#         print(imn)\n",
    "        data=np.array(a[imn][j])\n",
    "        v=np.var(data,axis=0)\n",
    "        va[i].append(np.mean(v))\n",
    "\n",
    "so={i:[] for i in range (4)}\n",
    "vv={i:[] for i in range (4)}\n",
    "for i in range (4):\n",
    "    so[i]=np.argsort(np.array(va[i])*-1)\n",
    "    vv[i]=np.sort(np.array(va[i])*-1)\n",
    "\n",
    "\n",
    "z=0\n",
    "idx={i:[] for i in range (len(trail)*4*189)}\n",
    "for i in range(4):\n",
    "    seq=so[i]\n",
    "    for j in seq:\n",
    "        for k in range (189):\n",
    "            n=trail[j]\n",
    "            idx[z].append(i)\n",
    "            idx[z].append(n)\n",
    "            idx[z].append(k)\n",
    "            z+=1\n",
    "\n",
    "index=shuffle(idx)\n",
    "\n",
    "\n",
    "def data_generator(idx, hdf5file,batch_size,if_train = True):\n",
    "    i = 0\n",
    "#     j=0 \n",
    "    while True:\n",
    "        X = []\n",
    "        Y=[]\n",
    "        for b in range(batch_size):\n",
    "            if i == len(idx):\n",
    "                i = 0\n",
    "            \n",
    "            alll=idx[i]\n",
    "            if alll[0]==0:\n",
    "                imn='images_30'\n",
    "                lbn='labels_30'\n",
    "\n",
    "            elif alll[0]==1:\n",
    "                imn='images_36'\n",
    "                lbn='labels_36'\n",
    "            elif alll[0]==2:\n",
    "                imn='images_46'\n",
    "                lbn='labels_46'\n",
    "            else:\n",
    "                imn='images_57'\n",
    "                lbn='labels_57'\n",
    "                \n",
    "            try:\n",
    "                x = (np.array(hdf5file[imn][alll[1]][alll[2]])/255).T  # read dataset on the fly\n",
    "                x=np.rollaxis(x,1,0)\n",
    "\n",
    "                y = np.array(hdf5file[lbn][alll[1]][alll[2]])  # read dataset on the fly\n",
    "\n",
    "\n",
    "                if np.sum(x)==0 or np.sum(x) == np.inf or np.sum(x) == -np.inf or np.sum(y)==0 or np.sum(y)==np.inf or np.sum(y)==-np.inf:\n",
    "                    x = (np.array(hdf5file[imn]['trial_0030'][60])/255).T  # read dataset on the fly\n",
    "                    x=np.rollaxis(x,1,0)\n",
    "                    y = np.array(hdf5file[lbn]['trial_0030'][60])  # read dataset on the fly\n",
    "            except:\n",
    "                x = (np.array(hdf5file[imn]['trial_0030'][60])/255).T  # read dataset on the fly\n",
    "                x=np.rollaxis(x,1,0)\n",
    "                y = np.array(hdf5file[lbn]['trial_0030'][60])  # read dataset on the fly\n",
    "            \n",
    "\n",
    "                \n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "\n",
    "            i += 1\n",
    "#             j += 1\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        Y = np.asarray(Y)\n",
    "        yield [X,Y],[X,Y]\n",
    "        \n",
    "# idx=np.arange(len(image))\n",
    "###############################################\n",
    "##################define classes for loss and regularizor######\n",
    "def Distance(x, y):\n",
    "    d = tf.reduce_sum(tf.square(x - y), 1)\n",
    "    dx = tf.maximum(d, 1e-9)\n",
    "    dist = tf.sqrt(dx)\n",
    "    return tf.reduce_mean(dist)\n",
    "\n",
    "\n",
    "def swiss_roll(size, noise=1.5):\n",
    "    u = K.random_uniform(shape=(size[0], 1),dtype=\"float32\")\n",
    "    u2 = K.random_uniform(shape=(size[0], 1),dtype=\"float32\")*10\n",
    "\n",
    "    t = 1.5 * np.pi * (1 + 1.5* u)\n",
    "    x = t * K.cos(t)#+11*K.ones_like(u2)\n",
    "    y =  u2 #+ 11*K.ones_like(u2)\n",
    "    z = t * K.sin(t) #+ 11*K.ones_like(u2)\n",
    "\n",
    "    X = K.concatenate([x,z], axis=-1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def squre_roll(size, noise=1.5):\n",
    "    u =K.random_uniform(shape=(size[0], 1),dtype=\"float32\")\n",
    "    u2 = K.random_uniform(shape=(size[0], 1),dtype=\"float32\")*10\n",
    "#     print(u)\n",
    "    t = 1.5 * np.pi * (1 + 100 * u)\n",
    "    x = t * K.cos(t)#+11*K.ones_like(u2)\n",
    "    y =  u2# + 11*K.ones_like(u2)\n",
    "    z = t * K.sin(t) #+ 11*K.ones_like(u2)\n",
    "\n",
    "    X = K.concatenate([x,y,z], axis=-1)\n",
    "\n",
    "    return X\n",
    "def spherical(size, ndim=3):\n",
    "    vec = K.random_normal(shape=(size[0], ndim),dtype=\"float32\")\n",
    "#     vec /= K.linalg.norm(vec, axis=0)\n",
    "    return vec\n",
    "\n",
    "def clusterdis(size, noise=1.5):\n",
    "    \n",
    "    a=int(size[0]/4)\n",
    "    b=a\n",
    "    c=a\n",
    "    d=size[0]-a-b-c\n",
    "    u = K.random_uniform(shape=(a, 3),dtype=\"float32\")\n",
    "    u2 = K.random_uniform(shape=(b, 3),dtype=\"float32\")*2\n",
    "    u3 = K.random_uniform(shape=(c, 3),dtype=\"float32\")*4\n",
    "    u4 = K.random_uniform(shape=(d, 3),dtype=\"float32\")*8\n",
    "#     sess = tf.Session()\n",
    "#     with sess.as_default():\n",
    "#         t=K.eval(size[0])\n",
    "    X = K.concatenate([u,u2,u3,u4], axis=0)\n",
    "\n",
    "#     X,_=make_blobs(n_samples=t, centers=4, n_features=3,random_state=42)\n",
    "    return X\n",
    "\n",
    "def GMM(size,noise=1.5):\n",
    "    \n",
    "    N,D = size[0], 3 # number of points and dimenstinality\n",
    "\n",
    "    means = np.array([[0.5, 0.0, 0.0],\n",
    "                      [0.0, 0.5, 0.8],\n",
    "                      [-0.5, -0.5, -0.5],\n",
    "                      [-0.8, 0.3, 0.4]])\n",
    "#     means= K.constant(means)\n",
    "\n",
    "    covs = np.array([np.diag([0.01, 0.01, 0.01]),\n",
    "                     np.diag([0.01, 0.01, 0.01]),\n",
    "                     np.diag([0.01, 0.01, 0.01]),\n",
    "                     np.diag([0.01, 0.01, 0.01])])\n",
    "#     covs=K.constant(covs)\n",
    "\n",
    "    n_gaussians = means.shape[0]\n",
    "\n",
    "    points = []\n",
    "    if N % 4 == 0:\n",
    "        L=int(N/4)\n",
    "        \n",
    "        \n",
    "    for i in range(len(means)):\n",
    "        x = np.random.multivariate_normal(means[i], covs[i],L)\n",
    "        points.append(x)\n",
    "\n",
    "    points=K.constant(np.concatenate(points))\n",
    "    \n",
    "    return points\n",
    "\n",
    "def get_kernel(X, Z,ksize):\n",
    "#     print(X.shape,Z.shape)\n",
    "    G = K.sum((K.expand_dims(X, axis=1) - Z)**2, axis=-1)  # Gram matrix\n",
    "    G = K.exp(-G/(ksize)) / (math.sqrt(2*np.pi*ksize)*K.ones_like(-G/(ksize)))\n",
    "    return G\n",
    "\n",
    "class DiagonalWeight(Constraint):\n",
    "    \"\"\"Constrains the weights to be diagonal.\n",
    "    \"\"\"\n",
    "    def __init__(self, N):\n",
    "        self.m = K.eye(N)\n",
    "        \n",
    "    def __call__(self, w):\n",
    "#         N = K.int_shape(w)[-1]\n",
    "#         m = K.eye(N)\n",
    "        w = w*self.m\n",
    "        return w\n",
    "    \n",
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch =  -0.5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) - K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(tf.reduce_mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs,tf.reduce_mean(kl_batch)\n",
    "\n",
    "class ITLRegularizer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds cs divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(ITLRegularizer, self).__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "    \n",
    "    def call(self, inputs,ks,flag,theta):\n",
    "\n",
    "        X = inputs\n",
    "#         print(X)\n",
    "        if flag=='sp':\n",
    "            Z=spherical(K.shape(X))\n",
    "        elif flag == 'cb':\n",
    "            Z=clusterdis(K.shape(X))\n",
    "        elif flag == 'gmm':\n",
    "        \n",
    "            Z=GMM(K.shape(X))\n",
    "        elif flag == 'sq':\n",
    "            Z=squre_roll(K.shape(X))\n",
    "        else:\n",
    "            Z=swiss_roll(K.shape(X))\n",
    "\n",
    "    \n",
    "\n",
    "#         Z=Z.astype(\"float32\")\n",
    "        ksize=ks\n",
    "        Gxx = get_kernel(X, X,ksize)\n",
    "        Gzz = get_kernel(Z, Z,ksize)\n",
    "        Gxz = get_kernel(X, Z,ksize)\n",
    "\n",
    "\n",
    "        r=K.log(K.sqrt(K.mean(Gxx)*K.mean(Gzz)+1e-5) /(K.mean(Gxz)+1e-5))\n",
    "\n",
    "        self.add_loss(r*theta, inputs=inputs)\n",
    "\n",
    "        return inputs,Z,r*theta\n",
    "    \n",
    "class OrthLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(OrthLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs,gamma):\n",
    "\n",
    "        A,B = inputs\n",
    "        U=K.concatenate((A,B),axis=1)\n",
    "        U = K.l2_normalize(U, axis=1)\n",
    "        batch = -K.dot(K.transpose(U),U) \n",
    "        I =tf.eye(K.shape(U)[1])#tf.Variable(lambda:K.eye(K.shape(U)[1])) #Lambda(lambda t: K.eye(t))()\n",
    "        batch=(batch+I)**2*gamma\n",
    "        self.add_loss(tf.reduce_mean(batch), inputs=inputs)\n",
    "\n",
    "        return inputs,tf.reduce_mean(batch)\n",
    "\n",
    "    \n",
    "    \n",
    "class MSE_SUP(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(MSE_SUP, self).__init__(*args, **kwargs)\n",
    "    def call(self,inputs, alpha):\n",
    "        D,A=inputs\n",
    "        L=tf.keras.losses.mse(D,A)\n",
    "        L=tf.reduce_mean(L)\n",
    "        \n",
    "        self.add_loss(L*alpha, inputs=inputs)\n",
    "        \n",
    "        return inputs,L*alpha\n",
    "        \n",
    "    \n",
    "class MSE_UNSUP(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(MSE_UNSUP, self).__init__(*args, **kwargs)\n",
    "    def call(self,inputs):\n",
    "        D,A=inputs\n",
    "\n",
    "        L=tf.keras.losses.mse(D,A)\n",
    "        L=tf.reduce_mean(L)\n",
    "        self.add_loss(L*128*128*2, inputs=inputs)\n",
    "        \n",
    "        return inputs,L*128*128*2\n",
    "    \n",
    "    \n",
    "LN2PI=np.log(2 * 3.1415926)\n",
    "\n",
    "\n",
    "\n",
    "class De_KL(Layer):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(De_KL, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def _gaussian_log_density_unsummed(self,z, mu, logvar):\n",
    "        \"\"\"First step of Gaussian log-density computation, without summing over dimensions.\n",
    "        Assumes a diagonal noise covariance matrix.\n",
    "        \"\"\"\n",
    "        diff_sq = (z - mu) ** 2\n",
    "        inv_var = K.exp(-logvar)\n",
    "        return - 0.5 * (inv_var * diff_sq + logvar + LN2PI)\n",
    "\n",
    "\n",
    "    def _gaussian_log_density_unsummed_std_normal(self,z):\n",
    "        \"\"\"First step of Gaussian log-density computation, without summing over dimensions.\n",
    "        Assumes a diagonal noise covariance matrix.\n",
    "        \"\"\"\n",
    "        diff_sq = z ** 2\n",
    "        return - 0.5 * (diff_sq + LN2PI)\n",
    "\n",
    "    \n",
    "    def _logsumexp(self,x, axis=None, keepdims=False):\n",
    "\n",
    "        x = tf.convert_to_tensor(x)\n",
    "        x_max = tf.math.reduce_max(x, axis=axis, keepdims=False)\n",
    "        temp=tf.math.reduce_sum(K.exp(x - x_max)+1, axis=axis,keepdims=True)\n",
    "        ret = K.log(tf.math.reduce_max(temp+1,tf.zeros_like(temp+1))) + x_max\n",
    "#         np.log(np.max(x, 1e-9))\n",
    "        if not keepdims:\n",
    "            ret = tf.math.reduce_sum(ret, axis=axis)\n",
    "#         print(K.sum(x,axis=1).numpy())\n",
    "        return ret#K.sum(x,axis=1)\n",
    "\n",
    "    \n",
    "    def call(self,inputs,beta):\n",
    "        z, mu, logvar=inputs\n",
    "        log_qz_prob = self._gaussian_log_density_unsummed(z[:, None], mu[None, :], logvar[None, :])\n",
    "        M =tf.nn.relu(K.sum(log_qz_prob, axis=2, keepdims=False))#,axis=1,keepdims=False)\n",
    "        c =tf.nn.relu(log_qz_prob)#,axis=1,keepdims=False)\n",
    "        t1=K.sum(K.exp(-M)+K.exp(K.sum(log_qz_prob, axis=2, keepdims=False)-M),axis=1,keepdims=False)\n",
    "        log_qz= K.log(t1+1e-5)\n",
    "        #tf.reduce_logsumexp(K.sum(log_qz_prob, axis=2, keepdims=False),axis=1,keepdims=False)\n",
    "       \n",
    "        \n",
    "        #self._logsumexp(K.sum(log_qz_prob, axis=2, keepdims=False),axis=1,keepdims=False)\n",
    "        \n",
    "        \n",
    "#         print(log_qz.shape)\n",
    "        \n",
    "        log_qz_ = tf.linalg.diag(M) # sum over gaussian dims\n",
    "        \n",
    "#         print(log_qz_.shape)\n",
    "        t2=K.sum(K.exp(-c)+K.exp(log_qz_prob-c),axis=1,keepdims=False)\n",
    "        log_qz_product = K.sum(#log_qz_prob,\n",
    "#             K.sum(K.log(K.exp(-c)+K.exp(log_qz_prob-c)),axis=1,keepdims=False),\n",
    "#             self._logsumexp(log_qz_prob, axis=1,keepdims=False),\n",
    "            K.log(t2+1e-5),\n",
    "#             tf.reduce_logsumexp(log_qz_prob, axis=1,keepdims=False),  # logsumexp over batch\n",
    "            axis=1,  # sum over gaussian dims\n",
    "            keepdims=False)\n",
    "        \n",
    "        log_pz_prob = self._gaussian_log_density_unsummed_std_normal(z)\n",
    "        log_pz_product = K.sum(log_pz_prob, axis=1, keepdims=False)  # sum over gaussian dims\n",
    "#         print(log_qz_.shape , log_qz.shape)\n",
    "        idx_code_mi = tf.experimental.numpy.nanmean(log_qz_ - log_qz)\n",
    "        total_corr = tf.experimental.numpy.nanmean(log_qz - log_qz_product)\n",
    "        dim_wise_kl = tf.experimental.numpy.nanmean(log_qz_product - log_pz_product)\n",
    "        idx_code_mi = tf.reduce_mean(idx_code_mi )\n",
    "        total_corr = tf.reduce_mean(total_corr )\n",
    "        dim_wise_kl = tf.reduce_mean(dim_wise_kl)\n",
    "        self.add_loss((idx_code_mi+total_corr *beta+dim_wise_kl+1e-5), inputs=inputs)\n",
    "        \n",
    "        return inputs,[idx_code_mi,total_corr *beta,dim_wise_kl]\n",
    "###############################################\n",
    "input0 = Input(shape=(128,128,2), name=\"mice1\")\n",
    "# encoder=Sequential()\n",
    "inputlabel=Input(shape=(sdim), name=\"micelabel1\")\n",
    "\n",
    "encoder=LeakyReLU(alpha=0.05)(Conv2D(32, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")(input0))\n",
    "encoder=BatchNormalization()(encoder)\n",
    "# print(encoder.shape)\n",
    "encoder=LeakyReLU(alpha=0.05)(Conv2D(64, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")(encoder))\n",
    "encoder=BatchNormalization()(encoder)\n",
    "# print(encoder.shape)\n",
    "encoder=LeakyReLU(alpha=0.05)(Conv2D(128, (5,5), strides=(2,2),padding='same', kernel_initializer=\"he_normal\")(encoder))\n",
    "encoder=BatchNormalization()(encoder)\n",
    "# print(encoder.shape)\n",
    "encoder=LeakyReLU(alpha=0.05)(Conv2D(256, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")(encoder))\n",
    "encoder=BatchNormalization()(encoder)\n",
    "# print(encoder.shape)\n",
    "encoder=LeakyReLU(alpha=0.05)(Conv2D(512, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")(encoder))\n",
    "encoder=BatchNormalization()(encoder)\n",
    "# print(encoder.shape)\n",
    "hidden=Flatten()(encoder)\n",
    "# hidden.shape\n",
    "\n",
    "z_mu = Dense(latent_dim,kernel_regularizer='l1_l2')(hidden)\n",
    "z_log_var = Dense(sdim+udim,kernel_regularizer='l1_l2')(hidden)\n",
    "#\n",
    "\n",
    "from scipy.stats import ortho_group\n",
    "m = ortho_group.rvs(dim=latent_dim).astype('float32')\n",
    "\n",
    "from scipy.linalg import qr\n",
    "\n",
    "n = sdim+udim+bac\n",
    "H = np.random.randn(n, n)\n",
    "Q, _ = qr(H)\n",
    "\n",
    "initializer = tf.keras.initializers.Orthogonal()\n",
    "initializera=tf.keras.initializers.Orthogonal(gain=1.0, seed=42)\n",
    "initializerb=tf.keras.initializers.Orthogonal(gain=2.0, seed=30)\n",
    "\n",
    "\n",
    "class FixWeights_sup(tf.keras.constraints.Constraint):\n",
    "\n",
    "    def __call__(self, w):\n",
    "        tf.keras.backend.set_value(w, Q[:sdim,:sdim])\n",
    "        return w\n",
    "class FixWeights_unsup(tf.keras.constraints.Constraint):\n",
    "\n",
    "    def __call__(self, w):\n",
    "        tf.keras.backend.set_value(w, Q[sdim:sdim+udim,sdim:sdim+udim])\n",
    "        return w\n",
    "    \n",
    "class FixWeights_bac(tf.keras.constraints.Constraint):\n",
    "\n",
    "    def __call__(self, w):\n",
    "        tf.keras.backend.set_value(w, Q[sdim+udim:sdim+udim+bac-1,sdim+udim:sdim+udim+bac-1])\n",
    "        return w\n",
    "\n",
    "C1 = Dense(bac, use_bias=False,kernel_regularizer='l1_l2')#(z_mu)\n",
    "A1 = Dense(sdim,use_bias=False,kernel_initializer=initializera,kernel_regularizer='l1_l2')#(z_mu)\n",
    "B1 = Dense(udim, use_bias=False, kernel_initializer=initializerb,kernel_regularizer='l1_l2')#(z_mu)\n",
    "\n",
    "\n",
    "z_log_var_A=z_log_var[:,:sdim]\n",
    "z_log_var_B=z_log_var[:,sdim:]\n",
    "\n",
    "\n",
    "\n",
    "A1.trainable = False\n",
    "B1.trainable = False\n",
    "\n",
    "A2 = A1(z_mu)\n",
    "B = B1(z_mu)\n",
    "C = C1(z_mu)\n",
    "\n",
    "C,dis,loss2= ITLRegularizer()(C,ks,flag,theta)\n",
    "\n",
    "D1=Dense(sdim,use_bias=True,kernel_constraint=DiagonalWeight(sdim), kernel_initializer=\"he_normal\")(A2)\n",
    "[D,A3],loss3=MSE_SUP()([D1,inputlabel],alpha)\n",
    "\n",
    "[A, z_log_var_A2],loss4 = KLDivergenceLayer()([A2, z_log_var[:,:sdim]])\n",
    "z_sigma_A = Lambda(lambda t: K.exp(.5*t))(z_log_var_A2)\n",
    "\n",
    "eps_A = K.random_normal(shape=(K.shape(input0)[0],sdim))\n",
    "z_eps_A2 = Multiply()([z_sigma_A, eps_A])#Multiply()([z_sigma_A, eps_A])\n",
    "z_A = Add()([A, z_eps_A2])\n",
    "\n",
    "z_sigma_B = Lambda(lambda t: K.exp(.5*t))(z_log_var[:,sdim:])\n",
    "eps_B = K.random_normal(shape=(K.shape(input0)[0],udim))\n",
    "z_eps_B = Multiply()([z_sigma_B, eps_B])\n",
    "z_B = Add()([B, z_eps_B])\n",
    "\n",
    "[z_B1, B1, z_log_var_B1],[loss5,loss6,loss7]= De_KL()([z_B, B, z_log_var_B],beta)\n",
    "\n",
    "\n",
    "alla=Concatenate()([z_A,z_B1,C])\n",
    "\n",
    "\n",
    "# print(zall.shape)\n",
    "decoder = Sequential()\n",
    "decoder.add(Dense(hidden.shape[1], kernel_initializer=\"he_normal\"))\n",
    "decoder.add(Reshape((4, 4, 512)))\n",
    "decoder.add(Conv2DTranspose(256, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\"))\n",
    "decoder.add(LeakyReLU(alpha=0.05))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "decoder.add(Conv2DTranspose(128, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\"))\n",
    "decoder.add(LeakyReLU(alpha=0.05))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "decoder.add(Conv2DTranspose(64, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\"))\n",
    "decoder.add(LeakyReLU(alpha=0.05))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "decoder.add(Conv2DTranspose(32, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\"))\n",
    "decoder.add(LeakyReLU(alpha=0.05))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "decoder.add(Conv2DTranspose(2, (5,5), strides= (2,2),activation='sigmoid',padding='same', kernel_initializer=\"he_normal\"))\n",
    "\n",
    "encmodel = Model(inputs=[input0,inputlabel], outputs=alla)\n",
    "\n",
    "out=decoder(alla)\n",
    "[out,_],loss8=MSE_UNSUP()([out,input0])\n",
    "\n",
    "\n",
    "\n",
    "allmodel= Model(inputs=[input0,inputlabel], outputs=[out,D])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001,clipvalue=1.0)\n",
    "\n",
    "allmodel.add_metric(loss2, \"csd_loss\")\n",
    "allmodel.add_metric(loss3, \"sup_loss\")\n",
    "allmodel.add_metric(loss4, \"kl_loss\")\n",
    "allmodel.add_metric(loss5, \"mi_loss\")\n",
    "allmodel.add_metric(loss6, \"total_corr\")\n",
    "allmodel.add_metric(loss7, \"dim_wise_kl\")\n",
    "allmodel.add_metric(loss8, \"UNSUP_loss\")\n",
    "\n",
    "allmodel.compile( optimizer=optimizer)\n",
    "\n",
    "\n",
    "callbacks=[LearningRateScheduler(lambda epoch: 0.001 * 0.85 ** (epoch // 5))]\n",
    "term=tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "\n",
    "\n",
    "history1 =allmodel.fit( data_generator(index, a,256,True),validation_data=data_generator(index[-10000:],a,100,False),\n",
    "                       batch_size=256, epochs=50,validation_steps=np.ceil(10000/100-1),\n",
    "                       verbose=1, steps_per_epoch=np.ceil((len(index))/256-1),\n",
    "                       callbacks=[term,callbacks])\n",
    "\n",
    "allmodel.save_weights('model.h5')\n",
    "\n",
    "# allmodel.load_weights('csdmodel_4_udim=2_alpha=1000_beta=5_gamma=1000_theta=500_ks=15_flag=sror3.h5')###!!!!!!!\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a824c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
